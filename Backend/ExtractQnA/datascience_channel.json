{
   "channelName": "Data Science SE",
   "channelContext": "This channel is designed to make questions regarding Data Science, it's from https://datascience.stackexchange.com/",
   "channelWikiTopics": ["Dataset", "Statistics", "Prediction", "Neural-Network", "Word-Embeddings", "NLP", "Entity Recognition"],
   "channelThreads": [
        {
            "threadMessage":"Does scaling required for this kind of datasets? I have a dataset with features like views of a product (in hundreds of thousands), clicks on the products (in thousands), conversion rate (in decimal such as 7.6%) and sales (in hundreds). Do I need to do a scaling of the data for clustering?",
            "threadReplies": [
                "It depends a lot on the data and the method you use. For instance, many clustering algorithms (e.g., K-Means) use a distance metric (Euclidean distance, Manhattan distance, etc.), so it's recommended to scale data. You may find handy : Do Clustering algorithms need feature scaling in the pre-processing stage? https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering"
            ]
        },
        {
            "threadMessage":"Generalization of accuracy score based on subset of data points I have a multi-class problem that I am building a classifier for. I have N total data points I would like to predict on. If I instead predict on n < N data points and get an accuracy score, is there a way I can say (with some degree of confidence) what I think the same model's accuracy score would be on the remaining data points? Can somebody point me to an article discussing this, or suggest a formula to research?",
            "threadReplies": [
                "If the smaller sample is representative of the bigger sample then yes, usually you would train a model undel the accuracy score stabilizes and doesn't change much and you would take this score as the approximate expected score.",
                "Usually when working with classification problems, one tries to have 3 subsets of data: A training set: this subset is usually the biggest one and can take up to ~80% of the available data. It is used to train the chosen algorithm, using the known labels of each data sample. A validation set: this subset is much smaller. It will typically be ~5-10% of the available data. It is used to evaluate the performance of the algorithm trained on the training set. Typically, one will tune the parameters of the algorithm in order to reach the best performance on the validation set. A testing set: this subset is of the same size order as the validation set or bigger. Very important: it should NEVER be used for training purpose! Once the model is trained and tuned using the training and validation sets, the testing set allows one to get the accuracy (or any other performance metric) on unseen data. If the model generalizes well, the score will be close to the ones seen on the validation set, often times a tiny bit worse. In order for this to work properly, it is important that all the subsets are representative of the available data. For example, that the proportion of each class is approximately the same across the subsets. In the light of such broadly used process, we can see that most algorithms are tuned and tested on a fraction of the available data. As long as the used test set is balanced in a way that is similar to the training and validation sets and that it has not been used at all during the training/tuning of the model, there is no reason why the performance scores would not generalize well to N > n samples used in the test set.",
                "Thanks. I am aware of train/validation/test best practices, but your statement \"there is no reason why the performance scores would not generalize well\" is in line with my actual question. Is it accurate to say there is no way to calculate the uncertainty associated with extrapolating the accuracy on the testing set to accuracy on a larger set drawn from the same distribution?"
            ]
        },
        {
            "threadMessage":"Dealing with multiple distinct-value categorical variables So, I've got a dataset with almost all of its columns are categorical variables. Problem is that most of the categorical variables have so many distinct values. For instance, one column have more than one million unique value, it's an IP address column in case anyone is interested. Someone suggested to split it into multiple other columns using domain knowledge, so split it to Network Class type, Host type and so on. However wouldn't that make my dataset lose some information? What if I wanted to deal with IP addresses as is? Nevertheless, the domain knowledge solution might work on the IP column, however, I've got other columns that have more than 100K distinct values, each value is a constant-length random string. I did work with Embedding Layers before, I was dealing with max thousands of features, never worked with 10K++ features, so I'm not sure if that would work with millions.  Much Regards",
            "threadReplies": [
                "Can you explain more about the problem you are trying to solve?",
                "Mainly, I'm trying to classify data according to some inputs, the inputs mainly constitute of categorical data, each categorical variable constitutes of so many distinct values. One of the independent variables is the IP address, which is essential for my classification problem. What I'm trying to do is to binary classify based on the (mostly categorical) inputs. Does that help? Let me know if you need more details.",
                "Embedding, Domain-based-features are most promising options here. For IP, it would be subnet ID, geo-location etc. Embedding works for large number of value (Such as word embedding for 10 Million+ words)",
                "What kind of information you are trying to extract from the IP?",
                "@ShamitVerma My dataset already contains countries, however, the country variable might be different than the IP country (usage of VPN's/proxies for instance). I didn't know that Embeddings work for data having millions of features actually, in that case that would be a reasonable solution for my question.",
                "@alirezazolanvari Not only the IP, along with multiple other values. Actually it's an old Kaggle problem that I'm trying to solve. I don't want to look for kernels (at least not yet) before trying to deal with the problem exactly as if it were real case scenario.",
                "Let's ask my question in another word. If you know what information you want extract from each categorical feature, it has some well-developed solutions. But if you don't know and just search for an algorithm which can extract the information of these features, its a bit hard",
                "@alirezazolanvari Are you referring to domain knowledge approach? Answering your question, what I'm looking for is the best representation of the given data in order to correlate statistically as possible with the hoped results without any over/under-fitting. Aren't the categorical features the information? Aren't they already describe a dataset and should correlate with the dependend variable in order to make sense? In other words, how can we extract information from an independent variable and why would I do so?",
                "I suggest word embedding algorithms. Because you should have an insight about how each feature can help you toward the favorable result.",
                "Have you heard of CatBoostClassifier? https://tech.yandex.com/catboost/doc/dg/concepts/python-reference_catboostclassifier-docpage/ It is type of Boosting classifier developed to deal specifically with categorical features. It has achieved state of the art results and the package developed by the authors have excellent support and even GPU portability. Take a look, this can be your solution."
            ]
        },
        {
            "threadMessage":"What is reliability? How it is related to correlation coefficient? We can compute real or population correlation(rho) by square-root of 1 minus R-squared.Is this a correct interpretation? Does population correlation mean a real correlation measured as square-root of 1 - R_squared? Here, Reliability seems to reflect extent of correlated-ness among variables/items (measuring a construct).Does it mean moderator effects/interaction-effects are eliminated by the this procedure to reach at true correlation?",
            "threadReplies": [
                "Could you please give the definition of reliability you mean?",
                "thanks for comment",
                "So could you please give me definition of reliability that you mean?",
                "There are several methods for ascertaining reliability e.g. split-half reliability. Please search and read the Net.",
                "I do not expect “search the net for the details of my question” to be a winning strategy for getting people to consult for free. You might want to consider fleshing out your question.",
                "Sorry, I did not have had such intention to win or lose."
            ]
        },
        {
            "threadMessage":"How to match a corpus with a string of words using a TF-IDF matrix? I am trying to match strings of words with a website that has bulletpoints whose text is most similar to it. The way I thought of doing it is to get all of the documents from each bulletpoint into one corpus per website, that I would like to match a string of words with, discard stop words, and then lemmatize everything. Then, for each string of text, I create a TF-IDF sparse matrix, with each row the text from a single bulletpoint from a single website, so that the matrix contains all the text from the bulletpoints from all the websites, as well as a row for the string of words I want to match. How should I then decide which row my string of words is most similar to? Should I get the cosine similarity of every row with my string of words row and just take whatever one has the highest cosine similarity (I will have a way of identifying the row with the website it was scrapped from)? Or is there an actual formalized way to go about this once I have my sparse matrix?",
            "threadReplies": [
                "I am trying to match strings of words with a website that has bulletpoints whose text is most similar to it. The way I thought of doing it is to get all of the documents from each bulletpoint into one corpus per website, that I would like to match a string of words with, discard stop words, and then lemmatize everything. Then, for each string of text, I create a TF-IDF sparse matrix, with each row the text from a single bulletpoint from a single website, so that the matrix contains all the text from the bulletpoints from all the websites, as well as a row for the string of words I want to match. How should I then decide which row my string of words is most similar to? Should I get the cosine similarity of every row with my string of words row and just take whatever one has the highest cosine similarity (I will have a way of identifying the row with the website it was scrapped from)? Or is there an actual formalized way to go about this once I have my sparse matrix?",
                "Thank you. How do I encode the input string s? What are weights in a TF-IDF matrix?",
                "@sangstar if you're using python you could encode the set of documents with TfidfVectorizer using fit_transform. Then to encode a new string s you would use transform, which applies the vocabulary and weights obtained before.",
                "The weights are the regular TFIDF values: for every word it's the product of its Term Frequency (TF) and Inverse Document Frequency (IDF) values. If you want you could implement your own functions to calculate the TFIDF matrix, it's not too complex and it's a good way to understand how TFIDF actually works.",
                "Ah, so I would obtain my weights with fit_transform and then can encode a new string with transform(s)? This will tell me which row had the highest similarity score?",
                "@sangstar No, this step only encodes s using the same encoding as the matrix of documents in order to make them comparable. Once this is done you still have to calculate the cosine similarity (for instance) between the encoded s and every document d in the matrix.",
                "Okay, so when it's encoded, is it now a row in my matrix? How do I reference it using TfidfVectorizer to find the cosine singularity between it and each predefined document row?",
                "@sangstar you can simply access the matrix by row, for example m[i] for accessing the doc at index i (see for instance here). If you don't have a lot of experience with sklearn and text, maybe you should start by familiarizing yourself with the concepts and methods. Also sklearn is not the only option, if you are used to a different language/library you should probably use that instead.",
                "I know that I can access a matrix by row with m[i] but which row of the TF-IDF matrix will contain the encoded s? Is it just appended as a new row at the bottom, so it's the last row, m[-1]? If I'm understanding you correctly, transform(s) encodes s into a feature vector, so that it can be used to find the cosine similarity with it and another vector. Are you saying it's not added to the matrix -- it just is converted to a feature vector with the encoding from the TF-IDF matrix?"
            ]
        },
        {
            "threadMessage":"What is the prefered way to tune several hyperparameters in cross-validation? Greedy vs alltogether? [closed] Should we tune all hyperparameters simultaneously, like in a multivariate optimization problem? Or one after the other, in a greedy way? The first option will produce better results but it will likely be less robust and much slower. Should we tune the number of epochs (early stopping) after all other hyperparameters? Should we use different validation data for each hyperparameter?",
            "threadReplies": [
                "Tune all hyperparameters together or not is depends on the resources available and model complexity. For example. This largely depends on the resources available and the complexity of your model. If computational resources are not a constraint, tuning all hyperparameters simultaneously is generally preferred, as it treats the problem as a multivariate optimization problem and can potentially lead to better results. However, this approach can be computationally expensive, especially for complex models with many hyperparameters. On the other hand, tuning one hyperparameter at a time (greedy approach) is less computationally intensive but may not always lead to optimal results because it does not consider interactions between different hyperparameters. Early stopping is typically used as a form of regularization to prevent overfitting during training. Usually people tune early stopping along with other hyperparameters because its performance can depend on them. Generally, you should use the same validation set when tuning different hyperparameters in order to maintain consistency in evaluation metrics across different configurations. However, if you have enough data, using K-fold cross-validation where you rotate your validation set can give more robust estimates of model performance.",
                "https://stackoverflow.com/help/gpt-policy"
            ]
        },
        {
            "threadMessage":"Loss Function for Probability Regression I am trying to predict a probability with a neural network, but having trouble figuring out which loss function is best. Cross entropy was my first thought, but other resources always talk about it in the context of a binary classification problem where the labels are {0,1}, but in my case I have an actual probability as the target. Is one of these options clearly best, or maybe are they all valid with just minor differences around the extreme 0/1 regions? Assuming x is the output of the final layer of my model. Cross Entropy: target∗−log(sigmoid(x))+(1−target)∗−log(1−sigmoid(x)) Mean Squared Error with Sigmoid: (sigmoid(x)−target)2 Mean Squared Error with Clamp: (x−target)2 When I use the output I clamp the values between [0,1].",
            "threadReplies": [
                "Cross entropy has been used in logistic regression for decades. Most applications of logistic regression are interested in the predicted probabilities, not developing decision procedures. So I think you're safe to go with cross-entropy.",
                "Is your target a single scalar that represents the probability or an array with each element represents the probabilities for each class?",
                "For example, if the goal is to predict the probability of an image contains a cat then your target will be a scalar. Alternatively, you could have a multiclass problem, in this case, we might be interested in predict the probility of an image contains a cat or a dog or a human. Then your target might be a array of probability (i.e [.1, .2., 7] each represents the probability for each class (notice the array adds up to 1)",
                "@mathew-drury: do you know of any resources talking about the non-classification case? For example it's weird to me that the cross entropy loss when predicting .7 when the target is .7 (ie a perfect prediction) is .61. I mean, the slope at that value is 0 which maybe is all that matters",
                "@LouisT: The target is a single scalar probability, not really a class though. For example you want a model that predicts the odds of drawing a matching card from a deck given a set of rules as inputs. So if you have the rule \"the card must be a heart\", the target would be .25.",
                "Do you mean that you want to predict probabilities instead of categories, or that your data set has probabilities as the responses instead of categories? (predictors,DOG) and (predictors,CAT) or (predictors,0.8) and (predictors,0.2)?",
                "Given that you are trying to predict a scalar probability value, the cross-entropy formula you listed in the question is only valid if the target variable is discrete. So if your question is \"predicts the odds of drawing a matching card from a deck\", it will be fine. The main difference between cross entropy and MSE will be how they penalize wrong predictions. Let's say given a target of 1 but prediction with 0. The cross-entropy is actually undefined in this case, but as the prediction gets closer to 0, the cross-entropy loss gets exponentially larger. On the other hand, your MSE is only 1. Which one is better, it depends on your application, if you want to avoid large margin of errors, it would seem the cross-entropy is more appropriate and vice versa. The second and third approach only differs in how they make sure the prediction is within [0, 1], one uses a sigmoid function and another uses a clamp. Given you are using a neural network, you should avoid using the clamp function. The clamp function is the same as the identity function within the clamped range, but completely flat outside of it. So the gradient of that function is 1 within the clamp range but 0 outside of it. Because of this, you are more likely to run into the dead neural problem in the same way when people talk about \"dead relu\"."
            ]
        },
        {
            "threadMessage":"How is WordPiece tokenization helpful to effectively deal with rare words problem in NLP? I have seen that NLP models such as BERT utilize WordPiece for tokenization. In WordPiece, we split the tokens like playing to play and ##ing. It is mentioned that it covers a wider spectrum of Out-Of-Vocabulary (OOV) words. Can someone please help me explain how WordPiece tokenization is actually done, and how it handles effectively helps to rare/OOV words?",
            "threadReplies": [
                "This question has been answered here. I'm copying the answer here as well. WordPiece and BPE are two similar and commonly used techniques to segment words into subword-level in NLP tasks. In both cases, the vocabulary is initialized with all the individual characters in the language, and then the most frequent/likely combinations of the symbols in the vocabulary are iteratively added to the vocabulary. Consider the WordPiece algorithm from the [original paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.p)",
                "The URL is broken. Here the corrected one https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf",
                "Here you go... youtube.com/watch?v=zJW57aCBCTk | I've recently started following this guy. He has a very nice way of explaining such concepts.",
                "WordPiece tokenization is a type of subword tokenization. Subword tokenization is a technique for splitting words into smaller units, called subwords, that are still meaningful. This is in contrast to traditional word tokenization, which simply splits words on whitespace or punctuation. WordPiece tokenization works by first creating a vocabulary of subwords. This vocabulary is created by iteratively merging the most common subwords together until the desired vocabulary size is reached. The merging process is done in a way that minimizes the information loss. When a word is tokenized using WordPiece, it is first pre-tokenized into words by splitting on whitespace and punctuation. Then, each word is tokenized into subwords using the vocabulary. If a word is not in the vocabulary, it is split into subwords using the merging process.                WordPiece tokenization is effective for handling rare/OOV words because it allows the model to learn the meaning of words even if they are not in the vocabulary. This is because the model can learn the meaning of a word from the subwords that make up the word. For example, the word \"playing\" is not in the vocabulary of a WordPiece model. However, the model can learn the meaning of the word by learning the meaning of the subwords \"play\" and \"ing\". WordPiece tokenization is a powerful technique for handling rare/OOV words in NLP models. It is used in many popular models, such as BERT, DistilBERT, and Electra."
            ]
        },
        {
            "threadMessage":"Decision Trees - C4.5 vs CART - rule sets When I read the scikit-learn user manual about Decision Trees, they mentioned that CART (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node. I don't understand where we compute rule sets for the C4.5 algorithm(and I dont even know what rule sets mean). Its essentially same as the CART, except that it uses gini index instead of cross entropy. Can someone please explain what rule sets are and how they are used in C4.5 in detail?",
            "threadReplies": [
                "I don't know exactly what the author means with that, but it looks like it refers to the rule-based post-pruning executed in C4.5 (I don't know how CART works to compare).",
                "Cart is binary tree. I think Defining rule set will not lead to optimization.",
                "@igrinis The link is no longer working.",
                "Decision Tree Algorithm: No matter which decision tree algorithm you are running: ID3, C4.5, CART, CHAID or Regression Trees(CART). They all look for the feature offering the highest information gain. Then, they add a decision rule for the found feature and build another decision tree for the sub-data set recursively until they reached a decision. C4.5 is the evolution of ID3, presented by the same author (Quinlan, 1993). The C4.5 algorithm generates a decision tree for a given dataset by recursively splitting the records. In building a decision tree we can deal with training sets that have records with unknown attribute values by evaluating the gain, or the gain ratio, for an attribute by considering only the records where that attribute is defined. In using a decision tree, we can classify records that have unknown attribute values by estimating the probability of the various possible results."
            ]
        },
        {
            "threadMessage":"I am trying to implement the AdaBoost.M1 algorithm (trees as base-learners) to a data set with a large feature space (~ 20.000 features) and ~ 100 samples in R. There exists a variety of different packages for this purpose; AdaBag, Ada and gbm. gbm() (from the gbm-package) appears to be my only available option, as stack.overflow is a problem in the others, and though it works, it is very time-consuming. Questions: Is there any way to overcome the stack.overflow the problem in the other packages, or have the gbm() run faster? I have tried converting the data.frame into a matrix without success. When performing AdaBoost in gbm() (with distribution set to \"AdaBoost\"), An Introduction to Statistical Learning (Hastie et al.) mentions the following parameters needed for tuning: Needs: The total number of trees to fit. The shrinkage parameter denoted lambda. The number of splits in each tree, controlling the complexity of the boosted ensemble. As the algorithm is very time consuming to perform in R, I need to find literature on what tuning parameters are within a suitable range for this kind of large feature space data, before performing cross-validation over the range to estimate the test error rate. Any suggestions?",
            "threadReplies": [
                "Is there any reason you have to use adaboost? gradient boosting (xgboost), as I know it, is more flexible and customization than adaboost and certainly faster in R. Here is an excellent answer on how to hypertune a gradient booster using the caret package: https://stats.stackexchange.com/questions/171043/how-to-tune-hyperparameters-of-xgboost-trees",
                "I believe that for such case with much more features then samples no method would work well, you should consider some feature space reduction with for example PCA"
            ]
        },
        {
            "threadMessage":"I have a large corpus of documents (web pages) collected from various sites of around 10k-30k chars each, I am processing them to extract relevant text as much as possible, but they are never perfect. Right now I creating a doc for each page, processing it with TFIDF and then creating a dense feature vector using UMAP. My final goal is to really pick out the differences in the articles, for similarity analysis, clustering and classification - however at this stage my goal is to generate the best possible embeddings. For this type of data what is the best method to create document embeddings? Also is it possible (is yes how) to embed various parts of the page; title, description, tags separately and them maybe combine this into a final vector?",
            "threadReplies": [
                "I have a large corpus of documents (web pages) collected from various sites of around 10k-30k chars each, I am processing them to extract relevant text as much as possible, but they are never perfect. Right now I creating a doc for each page, processing it with TFIDF and then creating a dense feature vector using UMAP. My final goal is to really pick out the differences in the articles, for similarity analysis, clustering and classification - however at this stage my goal is to generate the best possible embeddings. For this type of data what is the best method to create document embeddings? Also is it possible (is yes how) to embed various parts of the page; title, description, tags separately and them maybe combine this into a final vector?",
                "Transformer models such as BERT, DistilBert can be used to capture the document embeddings. Transformer models can capture the context more accurately than other models."
            ]
        },
        {
            "threadMessage":"Comparing transition matrices for Markov chains I have a population, each unit of which exists in one of several states that change over time. I am using first-order Markov chains to model these state transitions. My population can be segmented into various subpopulations of interest. I've obtained the transition matrices for each of these subpopulations and would like to know if these subpopulations differ from the general population in some principled way.            I don't know of any principled way of comparing transition matrices in this way. Comparing the transition matrices row-wise to that of the general population seems like one approach, but I'm not sure how to go about interpreting this. Another approach might be a spectral/eigendecomposition approach, which is much more readily interpretable to me, but might be harder to squeeze insight/stylised facts from. I've had a cursory search of the literature without much luck. Any suggestions?",
            "threadReplies": [
                "Continuing your first idea, you can use any discrete-probability distance-metric over the rows. Compare the rows of sub-population to the corresponding rows of the general population using measurements like Total Variation, KL divergence, Wasserstein, Kolmogorov-Smirnoff etc. You can read more to see what method fits you the most.",
                "Do you know anything about the distribution on states (i.e., what proportion of time is spent in each state)? If so, one way is to do a weighted average of the differences between corresponding row, weighted by the fraction of time spent in the state corresponding to that row. Did this make any sense? Do you have the same set of states for all subpopulations?",
                "That does make sense, and is applicable to my case. Thank you.",
                "What about using the eigenspectrum? It would be more focus on comparing the steady-states and not the transition but it looks to me like a sound solution.",
                "Does anybody know a scientific paper to support he comparison of Markov Chains using one of these techniques above?"
            ]
        },
        {
            "threadMessage":"Classify driver based on time-series sensor data I want to build a model that can detect which driver is driving now the car based on a dataset that contains 20 driver records for 3600s each driver ( the dataset contains all the car sensors values every second). So , now i have that dataset that contain the drivers records . How can i train a model that can identify the driver based on 60 seconds ( or more for example). Means , i want to make predictions with a dataframe of rows and not a single row . because we can't identify a driver with a single row. :)",
            "threadReplies": [
                "This is a very interesting problem. First of all I am looking forward to hearing what others have to say. It is a multivariate time series. If we look at it as a supervised learning, should not LSTM-RNN be able to help for multiclass-classification? My question is, how each driver is labeled? An exact driven name/id or a it is rather behavioral labeling? This is very much people at automotive industry are trying to look at.",
                "Yes , my first idea was to use LSTM-RNN and i'm looking for how to make data reliable for it ( convert dataframe to time sequences ) . Every driver has it's own ID ( or letter ) as label . ",
                "Have you seen the answer and suggested links in this post? https://datascience.stackexchange.com/questions/23196/multiclass-classification-on-live-sensor-data?rq=1 I think I pretty much have the very same dataset like yours. I think I wrote a function to sample efficiently (because the sensory data was collected every milliseconds) over the 3-min period; I can share this if you want. But I am not sure this is what you want at this stage. Is not your data already with timestamp? I would actually like to try these methods myself. Happy to collaborate as well if you like. ;-) ",
                "Sure , it will be great to collaborate with you :) . My data has a timestamp ( every second ) . So , your function must be helpful for me :) ."
            ]
        },
        {
            "threadMessage":"What are the reasons for drawing initial neural network weights from the Gaussian distribution? Are there theoretical or empirical reasons for drawing initial weights of a multilayer perceptron from a Gaussian rather than from, say, a Cauchy distribution?",
            "threadReplies": [
                "It seems to me that, by drawing weights from a Gaussian, units in upper layers will all tend to have similar dependencies to the inputs. ",
                "Related: https://datascience.stackexchange.com/questions/13061/when-to-use-he-or-glorot-normal-initialization-over-uniform-init-and-what-are",
                "The goal of initial weight distribution is get some amount of variance to allow learning to take place. Many distributions could work. Gaussian or uniform are the most commonly used. Cauchy distribution would not be a useful choice for several reasons. The primary reason is the Cauchy distribution is not as common as Gaussian or uniform distribution, thus is not as well supported in programming environments. Another reason could be Cauchy distribution's variance is undefined.",
                "There are different reasons. First of all, there is a general rule. Try to initialize your weights to be near zero, but avoid setting them to values that are too small. If you normalize your inputs and initialize your weights in this way, your cost function will somehow be a rounded cost function and it will be elongated. One way is to sample from the uniform and another way is to sample from a Gaussian. Uniform picks values in a range with the same probability, while the Gaussian chooses values near mean, zero, with more probability. Consequently, the Gaussian is better. One of the drawbacks of the usual Gaussian is that large values can be selected. In large networks, there are many parameters. So we may pick many weights with large values. large weight values are not good. They will lead to overfitting and will slow down the training process. So, people try to resample if a large value appears. To do so, they specify a threshold for the numbers. This approach is called the truncated method which can be applied to Gaussian distribution."
            ]
        },
        {
            "threadMessage":"Keras objective function shared between outputs Is there any way to implement a loss function that is shared between outputs? I have a 2D image output and scalar classification that are both used by a single loss function. I have attempted writing a function that returns a function, as in this comment, but I would need the input to the function to be the current training example. I also thought of using a merge layer, but that wouldn't work due to an incompatibility of the layer dimensions. Does anyone know of a way to write such a loss function in keras?",
            "threadReplies": [
                "Definitely possible in lower-level libraries - e.g. TensorFlow. So it will depend if Keras has a way to do it. For clarification, why do you need the full training example, usually the loss function takes prediction and ground truth (your problem here being how to represent that)?",
                "\"incompatibility of the layer dimensions when using merge\" - couldn't you use \"Flatten\" and then \"Merge\" with mode=concat?",
                "@NeilSlater I don't actually need the inputs for the training example, but I do need the ground truth and predictions for multiple branches. From what I see in keras, each branch has a different objective function. I was hoping to be able to do what I wanted easily, but it looks like I may have to implement an optimizer or make a lambda layer that captures everything. ",
                "@stmax I hadn't thought of that, but it does seem like a possibility. I found an approach using a Lambda layer in one of the examples that looks like it may work. ",
                "@danielunderwood could you please post how you solve this problem? Would love to see some code.",
                "@SrikarAppalaraju I just looked back through the code for this project and it seems that I switched to tensorflow and tensorlayer due in part to the trouble with implementing this loss. It looks like there's an issue on GitHub that may contain a solution using lambdas."
            ]
        },
        {
            "threadMessage":"Fixed-radius range search in non-Euclidean space I'm trying to find an indexing data structure most suitable for my metric space: set of IP network related data (IP addresses, ports, TCP flags, ...),            distance function is continuous, non-Euclidean but satisfies non-negativity, symmetry and triangle inequality,            with respect to the fixed-radius range query performance. I want to use it for a clustering algorithm (DBSCAN or similar) on a data sets with millions of elements. So far I have studied:                        ball trees/VPT,     MVPT,            GHT, GNAT,            AESA/LAESA,            cover trees,            and basically all other methods from Searching in metric spaces survey, but all mentioned are suited for varying-radius queries. Many related SO answers advise Locality-sensitive hashing (LSH) as a current trend, but I didn't find any clear information whether it exploits the advantage of fixed-range and whether it is usable for non-Euclidean metrics. Any advices/references?",
            "threadReplies": [
                "Try some of the indexes in ELKI to see what works best for your distance function...",
                "@Anony-Mousse In ELKI there are only cover trees and M-trees metrical indexes. Others (including LSH) can be used with certain subset of distance functions only. ",
                "Yes, LSH etc. need specializations for every distance function. So you'll have to do this yourself."
            ]
        },
        {
            "threadMessage":"Yes, LSH etc. need specializations for every distance function. So you'll have to do this yourself. My data has two columns and both are highly correlated e.g. if column1 has value ABC, column2 should be XYZ i.e. ABC-->XYZ. If column2 has anything else it's Anomaly. Likewise, there are thousands of combinations. I already tried KModes clustering where a number of clusters = unique values in column1. However, each cluster does not have equal density hence some bad data with high density is classified as normal and good data with low density is marked anomalous. I want to have unsupervised algo where I can force it to use column1 as the primary criteria for clustering. One with the highest frequency of column2 data for each unique value of column1 is good data. Rest is anomalous. Kindly suggest what would be the best algo and how to approach this problem.",
            "threadReplies": [
                "Clustering is likely to fail. There is little use in relying on heuristics like kmeans when the patterns are that obvious. Juat identify the common values (ABC and XYZ are supposedly frequent) by counting not by clustering, and label everything else as anomalous.",
                "Although I am using KModes (not KMeans), your idea of simple counting and label low frequency pattern as outliers definitely makes sense. I will try it out",
                "One option is counting patterns. Then define less common occurring patterns as an anomalies.                The counting approach is deterministic, whereas clustering is probabilistic. It might solve your problem. If not, it will at least provide summary statistics and a baseline model."
            ]
        },
        {
            "threadMessage":"Metrics to evaluate features' importance in classification problem (with random forest) I want to evaluate the importance of each of the features of a 2000x60 dataset in a classification problem with random forest. The most widely used ones apparrently are: Cross Entropy-Information Gain            Gini Importance (SkLearn implementation with feature_importances_)            Mean Squared Error (H2O implementation with h2o.varimp)            I have also found a rather concise overview of some other metrics for variables' importance at random forests at this research paper.                        These are the following:            Altmann            Boruta            Permutation            Recurrent relative variable importance            Recursive feature elimination            Vita            VSURF            Has anyone used these and which one was the most informative for his/her model?            Do you have any other metrics of this kind for variable importance at random forests?",
            "threadReplies": [
                "Have you seen github.com/slundberg/shap? A quick intro.: https://medium.com/civis-analytics/demystifying-black-box-models-with-shap-value-analysis-3e20b536fc80",
                "Thank you for your comment. As we can see there are multiple method to something like this. I am more interested to know which ones have been used personally by you or by data scientists in general and which ones performed the best. Have you used any of the aforementioned methods? How did they perform?",
                "One of the most commonly used methods is permutation feature importance which is decrease in a model score when a single feature value is randomly shuffled. The result is ranked ordered list of features and with each one having a distributional estimates. It works very well with Random Forests.",
                "Microsoft's SHAP values are very interesting and seem to bridge the gaps between LIME, Input Gradients, Global Surrogate, and basic permutation feature importance. In addition to being fairly comprehensible, Slundberg's library is very comprehensive and has got some great visualizations tools for your model."
            ]
        },
        {
            "threadMessage":"Dealing with categorical variables in Isolation Forest Isolation Forest is widely used when dealing with outlier/anomaly detection when we have no labels. The theory behind is that making random split at random points and counting how many splits you do to isolate a feature will help you determine if an instance is or not an outlier. I have categorical features and I am not sure how to deal with them:  Label Encoding: Will misrepresent the data in euclidean space. One Hot Encoding: Will give me more features and since the source code first selects the columns and then the values, it will give a non-realistic probability for my algorithm to select the one hot encoded Target Encoding wont work since we have no target How to properly encode categorical features in Isolation Forest? Could we encode categorical features in a space that suits the algorithm",
            "threadReplies": [
                "How about other unsupervised ways of encoding e.g. Binary, Count, Hashing etc. (avaiable at github.com/scikit-learn-contrib/category_encoders)? I think Catboost uses some of these methods to encode categorical variables automatically, well that is an gradient boosting trees, but I am mentioning so that you can use them too in Isolation Forest.",
                "Yes, there is a lot of categoy encoders. My question is what is the best and why? Catboost Encoded is target encoding but with some fancy functionalities, but at the end target encoding, so it requires a label",
                "Catboost has more than just target encoding but that is beyond the point. Then your Q is not clear. What is the best way to encode either supervised or unsupervised is general question in many algorithms. Better to redefine the question 'How to properly encode categorical features in Isolation Forest?' by adding at the end 'in order to..'. What is the criteria here? Those cons you listed are generic issues with such encoding methods and not limited to IF.",
                "I understand that IF is an algorithm for outlier/anomaly detection. Thanks for the edit suggestion",
                "@TwinPenguins in the catboost encoding it says \"This is very similar to leave-one-out encoding,\" and in the leave one out documentation \"This is very similar to target encoding \"",
                "@TwinPenguins arxiv.org/abs/2105.13783",
                "Your own paper/method, bravo! Will have a look. Is it already included in sktools?",
                "It is included in sktools, also in category_encoders. Even thought category_encoders needs to do a package release to be able to use it."
            ]
        },
        {
            "threadMessage":"Is there a way to rank the Extracted Named Entities based on their importance/occurence in a document? Looking for a way to rank the tens and hundreds of named entities present in any document in order of their importance/relevance in the context. Any thoughts ? Thanks in advance!",
            "threadReplies": [
                "An easy way would be to use TF-IDF (term frequency–inverse document frequency). It can help you find how much terms stand out in a document (by comparing with your entire corpus) and use it to rank your entities. TfidfVectorizer from scikit-learn Just note that the TfidfVectorizer is on a word level. So some processing will be needed if your entities can consist of more than one word. Alternatively you could use a model that allows you to produce a heatmap of the words. Then you can use that heatmap to look up your NEs in that heatmap. This paper, A Structured Self-Attentive Sentence Embedding, could give you some ideas.",
                "Thanks for your response, Simon! However, I was wondering if there is a better approach since tf-idf will not take into consideration the context of the document.",
                "You are correct. I think you can achieve what you are looking for by using a self-attention model. I will link something in my answer."
            ]
        }
    ]
}
